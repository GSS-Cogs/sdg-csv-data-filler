{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from yaml import safe_load, dump\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sdg-csv-data-filler is the first module in a data pipeline to take\n",
    "data from the SDG data repo and make it exportable as CSVW.\n",
    "\"\"\"\n",
    "\n",
    "# setting paths to directories and files\n",
    "remote_data_url = \"https://github.com/ONSdigital/sdg-data/tree/develop/data\"\n",
    "cwd = os.getcwd()\n",
    "data_path = os.path.join(cwd, 'data')\n",
    "out_path = os.path.join(cwd, 'out')\n",
    "gap_filler_yam_path = (os.path.join\n",
    "                       (\"substitutions\",\n",
    "                        \"gap_filler.yaml\"))\n",
    "header_mapping_yam_path = (os.path.join\n",
    "                           (\"substitutions\",\n",
    "                            \"header_mapping.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_dicts(path_to_yaml):\n",
    "    \"\"\"\n",
    "    Loads dictionaries for the gap filling and mapping of column names \n",
    "    from locally stored .yaml files\n",
    "    \n",
    "        Parameters:\n",
    "            gap_filler_yaml (string): Path to the yaml file storing the values\n",
    "                to fill gaps with in each column\n",
    "            header_mapping_yaml (string): Path to the yaml file storing the \n",
    "                names to change headers to for each column\n",
    "        Returns:\n",
    "            dict: dict_from_yam\n",
    "            \n",
    "    \"\"\"\n",
    "    with open(path_to_yaml) as file:\n",
    "        dict_from_yam = safe_load(file)\n",
    "    \n",
    "    return dict_from_yam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_urls(url):\n",
    "    \"\"\"\n",
    "    Provided with a data folder URL, this function finds the URLS\n",
    "    of the CSV files within the folder. A generator is yielded with\n",
    "    the links of all files in the folder.\n",
    "        Parameters:\n",
    "            url (string): the URL of the repo/folder which contains\n",
    "                the CSV files to be captured\n",
    "        Yields:\n",
    "            string: generator, the next URL for the CSV file in the \n",
    "            remote data folder \n",
    "    \"\"\"\n",
    "    page = requests.get(url) #proxies=proxies\n",
    "    soup = bs(page.text, 'html.parser')\n",
    "    csv_link_pattern = r\"\\/ONSdigital\\/sdg-data\\/blob\\/develop\\/data\\/indicator_\\d-\\d{1,2}-\\d{1,2}\\.csv\"\n",
    "    to_repl_pattern = r\"\\/sdg-data\\/blob\\/develop\"\n",
    "    replacement_pattern = \"/sdg-data/develop\"\n",
    "    list_of_links = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(csv_link_pattern)}):\n",
    "        link = link.get('href')\n",
    "        link = re.sub(to_repl_pattern, replacement_pattern, link)\n",
    "        yield (\"https://raw.githubusercontent.com\"+link)\n",
    "        # Alternative to return a list\n",
    "        # list_of_links.append(\"https://raw.githubusercontent.com\"+link)\n",
    "    # return list_of_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvs_to_pandas(url):\n",
    "    \"\"\"\n",
    "    Provided with a URL of a file, the fucntion will check if the CSV\n",
    "    is populated and if not empty return a Pandas dataframe of the CSV\n",
    "        Parameters:\n",
    "            url (string): the URL of a CSV file to be captured\n",
    "        Returns:\n",
    "            pd.DataFrame: a Pandas dataframe of the CSV\n",
    "    \"\"\"\n",
    "    if \"no data for this indicator yet\" in str(bs(requests.get(url).text)):\n",
    "        return None\n",
    "    else:\n",
    "        return pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvsample_to_pandas(path_to_file, pct=1.0):\n",
    "    \"\"\"\n",
    "    A function to create a sample extract of a csv as a dataframe\n",
    "    \n",
    "        Parameters:\n",
    "            path_to_file (string): full path to csv file\n",
    "            p (float): decimal amount of lines to extract\n",
    "            \n",
    "        Returns:\n",
    "            pd.Dataframe\n",
    "            \"\"\"\n",
    "    p = pct/100  \n",
    "    # keep the header, then take only 1% of lines\n",
    "    # if random from [0,1] interval is greater than 0.01 the row will be skipped\n",
    "    df = pd.read_csv(\n",
    "             path_to_file,\n",
    "             header=0, \n",
    "             skiprows=lambda i: i>0 and random.random() > p)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(df, gap_filler_dict):\n",
    "    \"\"\"\n",
    "    Given a Pandas dataframe and a dictionary containing the column names\n",
    "    the correct 'fillers' for each column, this function will fill\n",
    "    each column with the correct values when empty cells are found.\n",
    "        Parameters:\n",
    "            pd_df (pd.Dataframe): the variable to which the dataframe \n",
    "                containing the csv data is assigned\n",
    "            gap_filler_dict (dict): a dictionary with column name and value \n",
    "                to fill gaps as key value pairs, e.g.\n",
    "                {\"Age\":\"All\",\"Sex\":\"T\"}\n",
    "        Returns:\n",
    "            pd.Dataframe: A dataframe with all cells full\"\"\"\n",
    "    df = df.fillna(gap_filler_dict, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_cell_values(pd_df, dict_of_nonstandard_standard):\n",
    "    \"\"\"\n",
    "    Maps non-standard values e.g. \"Males\" to standard values like \"M\".\n",
    "    Mapping is carried out on a column-specific basis.\n",
    "    \"\"\"\n",
    "    df = (pd_df.replace\n",
    "          (to_replace=dict_of_nonstandard_standard,\n",
    "          value=None))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(df, out_path, filename):\n",
    "    \"\"\"\n",
    "    Converts a Pandas dataframe to CSV and writes it out to a local folder.\n",
    "        Parameters:\n",
    "            pd_df (pd.Dataframe): The pandas data frame of the data\n",
    "            path (string): the path of the local \"out\" folder\n",
    "        Returns:\n",
    "            Boolean: True is written, False if not written \"\"\" \n",
    "    status = True\n",
    "\n",
    "    # If the out_path isn't there, make it\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        full_path = os.path.join(out_path, filename)\n",
    "        df.to_csv(csv_dir, index=False)\n",
    "    except Exception as e:\n",
    "        extended_e = Exception(\"Error encountered when attempting csv write to csv_dir: \".format(csv_dir)) from e\n",
    "        print(extended_e)\n",
    "        return False\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def entry_point(data_url):\n",
    "#     urls_gen = find_csv_urls(data_url)\n",
    "#     gap_filler_dict = (get_mapping_dicts(gap_filler_yam_path) \n",
    "#     header_mapping_dict = (get_mapping_dicts(header_mapping_yam_path))\n",
    "\n",
    "#     for _url in urls_gen:\n",
    "#         data_name = extract_name(_url)\n",
    "#         df = csvs_to_pandas(_url)\n",
    "#         if not df:\n",
    "#             continue\n",
    "#         df = fill_gaps(df, gap_filler_dict)\n",
    "#         df = standardise_headers(df)\n",
    "#         write_csv(df, out_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     entry_point(data_url=remote_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_random_values(df, holes=20):\n",
    "    \"\"\"\n",
    "    Smashes holes in your dataframe to the approximate number that you\n",
    "    request (randint might choose the same cell twice)\n",
    "    \"\"\"\n",
    "    for i in range(holes):\n",
    "        row = random.randint(1, df.shape[0]-1)\n",
    "        col = random.randint(0, df.shape[1]-1)\n",
    "        df.iloc[row, col] = float('nan')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_yam_path = os.path.join(os.getcwd(), \"substitutions\", \"samp_filler.yaml\")\n",
    "csv_path = os.path.join(os.getcwd(), \"data\", \"indicator_9-1-1.csv\")\n",
    "dict_of_nonstandard_standard_path = (os.path.join(os.getcwd(),\n",
    "                                                  \"substitutions\",\n",
    "                                                  \"test_dict_of_nonstandard_standard.yaml\"))\n",
    "\n",
    "def proof_of_concept(csv_path,\n",
    "                     ns=dict_of_nonstandard_standard_path, \n",
    "                     path=test_yam_path,\n",
    "                     out_path=out_path,\n",
    "                     file_name=\"test_df.csv\"):\n",
    "    # Creating a sample df. This is in place of csvs_to_pandas\n",
    "    samp_df = csvsample_to_pandas(path_to_file=csv_path, pct=1)\n",
    "    # Testing get_mapping_dicts func. Creating filler dictioary\n",
    "    samp_filler_dict = get_mapping_dicts(path)\n",
    "    # Creating dictionary to map non-standard terms with standard ones. \n",
    "    nonstandard_standard = get_mapping_dicts(ns)\n",
    "    # Creating gaps in the data (this will not be used in production)\n",
    "    holey_df = delete_random_values(samp_df)\n",
    "    # Testing that the filler dict works\n",
    "    refilled_df = holey_df.fillna(value=samp_filler_dict)\n",
    "    # Testing that standardise dict dills gaps as expected\n",
    "    samp_df = standardise_cell_values(refilled_df, nonstandard_standard)\n",
    "    #Writing the df to csv locally. \n",
    "    was_written = write_csv(samp_df, out_path, 'sample.csv')\n",
    "    return samp_df, was_written\n",
    "\n",
    "poc_df, was_written = proof_of_concept(csv_path, path=test_yam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "was_written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipdb in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (0.13.3)\n",
      "Requirement already satisfied: setuptools in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipdb) (49.6.0.post20200814)\n",
      "Requirement already satisfied: ipython>=5.1.0; python_version >= \"3.4\" in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipdb) (7.16.1)\n",
      "Requirement already satisfied: pygments in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.3.3)\n",
      "Requirement already satisfied: decorator in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.17.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (3.0.5)\n",
      "Requirement already satisfied: backcall in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.8.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n",
      "Requirement already satisfied: six in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.15.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from jedi>=0.10->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.1)\n",
      "Requirement already satisfied: wcwidth in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/james/anaconda3/envs/sdg-csvw/lib/python3.8/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write = \"\"\n",
    "remote_data_url = \"https://github.com/ONSdigital/sdg-data/tree/develop/data\"\n",
    "csv_gen = find_csv_urls(remote_data_url)\n",
    "\n",
    "\n",
    "for url in csv_gen:\n",
    "    if \"no data for this indicator yet\" in str(bs(requests.get(url).text)):\n",
    "        continue\n",
    "    yam_txt=f\"{url} : \\n\"\n",
    "    df = pd.read_csv(url)\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "    header_overide_txt = \"\"\n",
    "    column_specific_value_override=\"\"\n",
    "    for column in df.columns:\n",
    "        header_overide_txt+=f\"\"\"#Header override for \\'{column}\\'\n",
    "    {column} :\n",
    "        to : correct_header_for_{column.replace(\" \",\"_\")}\\n\"\"\"\n",
    "        column_specific_value_override+=f\"\"\"#Value overrides for column \\'{column}\\'\n",
    "        FILL_NA: filler_value\n",
    "        OldValue1: NewValue1\n",
    "        OldValue2: NewValue2\n",
    "        OldValue3: NewValue3\\n\\n\"\"\"\n",
    "        yam_txt +=f\"\"\"\n",
    "        {header_overide_txt}\n",
    "        {column_specific_value_override}\"\"\"\n",
    "    to_write += yam_txt\n",
    "\n",
    "with open('overrides.yaml', 'w+') as y:\n",
    "    y.write(to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yaml dump version\n",
    "csv_gen = find_csv_urls(remote_data_url)\n",
    "yam_dict={}\n",
    "for url in csv_gen:\n",
    "    if \"no data for this indicator yet\" in str(bs(requests.get(url).text)):\n",
    "        continue\n",
    "    df = pd.read_csv(url)\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "    column_specific_value_dict= {}\n",
    "    for column in df.columns:\n",
    "        if column == 'Value':\n",
    "            continue\n",
    "        column_specific_value_dict[column]= {'to' : f'correct_header_for_{column.replace(\" \",\"_\")}',\n",
    "            'FILL_NA' : f'{column.replace(\" \",\"_\")}_gap_filler_value',\n",
    "            'OldValue1' : 'NewValue1',\n",
    "            'OldValue2' : 'NewValue2',\n",
    "            'OldValue3' : 'NewValue3'}\n",
    "    yam_dict[url] = column_specific_value_dict\n",
    "\n",
    "    \n",
    "    \n",
    "with open('overrides_dict.yaml', 'w') as yam:\n",
    "    dump(yam_dict, yam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://raw.githubusercontent.com/ONSdigital/sdg-data/develop/data/indicator_9-5-2.csv': {'Year': {'to': 'correct_header_for_Year',\n",
       "   'FILL_NA': 'Year_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Units': {'to': 'correct_header_for_Units',\n",
       "   'FILL_NA': 'Units_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Disability status': {'to': 'correct_header_for_Disability_status',\n",
       "   'FILL_NA': 'Disability_status_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Age': {'to': 'correct_header_for_Age',\n",
       "   'FILL_NA': 'Age_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Sex': {'to': 'correct_header_for_Sex',\n",
       "   'FILL_NA': 'Sex_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Region': {'to': 'correct_header_for_Region',\n",
       "   'FILL_NA': 'Region_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Country of birth': {'to': 'correct_header_for_Country_of_birth',\n",
       "   'FILL_NA': 'Country_of_birth_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Nationality': {'to': 'correct_header_for_Nationality',\n",
       "   'FILL_NA': 'Nationality_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Unit measure': {'to': 'correct_header_for_Unit_measure',\n",
       "   'FILL_NA': 'Unit_measure_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Unit multiplier': {'to': 'correct_header_for_Unit_multiplier',\n",
       "   'FILL_NA': 'Unit_multiplier_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Observation status': {'to': 'correct_header_for_Observation_status',\n",
       "   'FILL_NA': 'Observation_status_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'GeoCode': {'to': 'correct_header_for_GeoCode',\n",
       "   'FILL_NA': 'GeoCode_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'},\n",
       "  'Value': {'to': 'correct_header_for_Value',\n",
       "   'FILL_NA': 'Value_gap_filler_value',\n",
       "   'OldValue1': 'NewValue1',\n",
       "   'OldValue2': 'NewValue2',\n",
       "   'OldValue3': 'NewValue3'}}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yam_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
