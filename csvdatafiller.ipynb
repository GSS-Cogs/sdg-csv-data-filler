{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from yaml import safe_load\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sdg-csv-data-filler is the first module in a data pipeline to take\n",
    "data from the SDG data repo and make it exportable as CSVW.\n",
    "\"\"\"\n",
    "\n",
    "# setting paths to directories and files\n",
    "remote_data_url = \"https://github.com/ONSdigital/sdg-data/tree/develop/data\"\n",
    "cwd = os.getcwd()\n",
    "data_path = os.path.join(cwd, 'data')\n",
    "out_path = os.path.join(cwd, 'out')\n",
    "gap_filler_yam_path = (os.path.join\n",
    "                       (\"substitutions\",\n",
    "                        \"gap_filler.yaml\"))\n",
    "header_mapping_yam_path = (os.path.join\n",
    "                           (\"substitutions\",\n",
    "                            \"header_mapping.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_dicts(path_to_yaml):\n",
    "    \"\"\"\n",
    "    Loads dictionaries for the gap filling and mapping of column names \n",
    "    from locally stored .yaml files\n",
    "    \n",
    "        Parameters:\n",
    "            gap_filler_yaml (string): Path to the yaml file storing the values\n",
    "                to fill gaps with in each column\n",
    "            header_mapping_yaml (string): Path to the yaml file storing the \n",
    "                names to change headers to for each column\n",
    "        Returns:\n",
    "            dict: dict_from_yam\n",
    "            \n",
    "    \"\"\"\n",
    "    with open(path_to_yaml) as file:\n",
    "        dict_from_yam = safe_load(file)\n",
    "    \n",
    "    return dict_from_yam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_urls(url):\n",
    "    \"\"\"\n",
    "    Provided with a data folder URL, this function finds the URLS\n",
    "    of the CSV files within the folder. A generator is yielded with\n",
    "    the links of all files in the folder.\n",
    "        Parameters:\n",
    "            url (string): the URL of the repo/folder which contains\n",
    "                the CSV files to be captured\n",
    "        Yields:\n",
    "            string: generator, the next URL for the CSV file in the \n",
    "            remote data folder \n",
    "    \"\"\"\n",
    "    page = requests.get(url, verify=False) #proxies=proxies\n",
    "    soup = bs(page.text, 'html.parser')\n",
    "    csv_link_pattern = r\"\\/ONSdigital\\/sdg-data\\/blob\\/develop\\/data\\/indicator_\\d-\\d{1,2}-\\d{1,2}\\.csv\"\n",
    "    to_repl_pattern = r\"\\/sdg-data\\/blob\\/develop\"\n",
    "    replacement_pattern = \"/sdg-data/develop\"\n",
    "    list_of_links = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(csv_link_pattern)}):\n",
    "        link = link.get('href')\n",
    "        link = re.sub(to_repl_pattern, replacement_pattern, link)\n",
    "        yield (\"https://raw.githubusercontent.com\"+link)\n",
    "        # Alternative to return a list\n",
    "        # list_of_links.append(\"https://raw.githubusercontent.com\"+link)\n",
    "    # return list_of_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvs_to_pandas(url):\n",
    "    \"\"\"\n",
    "    Provided with a URL of a file, the fucntion will check if the CSV\n",
    "    is populated and if not empty return a Pandas dataframe of the CSV\n",
    "        Parameters:\n",
    "            url (string): the URL of a CSV file to be captured\n",
    "        Returns:\n",
    "            pd.DataFrame: a Pandas dataframe of the CSV\n",
    "    \"\"\"\n",
    "    if \"no data for this indicator yet\" in str(bs(requests.get(url).text)):\n",
    "        return None\n",
    "    else:\n",
    "        return pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvsample_to_pandas(path_to_file, pct=1.0):\n",
    "    \"\"\"\n",
    "    A function to create a sample extract of a csv as a dataframe\n",
    "    \n",
    "        Parameters:\n",
    "            path_to_file (string): full path to csv file\n",
    "            p (float): decimal amount of lines to extract\n",
    "            \n",
    "        Returns:\n",
    "            pd.Dataframe\n",
    "            \"\"\"\n",
    "    p = pct/100  \n",
    "    # keep the header, then take only 1% of lines\n",
    "    # if random from [0,1] interval is greater than 0.01 the row will be skipped\n",
    "    df = pd.read_csv(\n",
    "             path_to_file,\n",
    "             header=0, \n",
    "             skiprows=lambda i: i>0 and random.random() > p)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(pd_df, gap_filler_dict):\n",
    "    \"\"\"\n",
    "    Given a Pandas dataframe and a dictionary containing the column names\n",
    "    the correct 'fillers' for each column, this function will fill\n",
    "    each column with the correct values when empty cells are found.\n",
    "        Parameters:\n",
    "            pd_df (pd.Dataframe): the variable to which the dataframe \n",
    "                containing the csv data is assigned\n",
    "            gap_filler_dict (dict): a dictionary with column name and value \n",
    "                to fill gaps as key value pairs, e.g.\n",
    "                {\"Age\":\"All\",\"Sex\":\"T\"}\n",
    "        Returns:\n",
    "            pd.Dataframe: A dataframe with all cells full\"\"\"\n",
    "    df = pd_df.fillna(gap_filler_dict, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_cell_values(pd_df, dict_of_nonstandard_standard):\n",
    "    \"\"\"\n",
    "    Maps non-standard values e.g. \"Males\" to standard values like \"M\".\n",
    "    Mapping is carried out on a column-specific basis.\n",
    "    \"\"\"\n",
    "    df = (pd_df.replace\n",
    "          (to_replace=dict_of_nonstandard_standard,\n",
    "          value=None))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(df, out_path):\n",
    "    \"\"\"\n",
    "    Converts a Pandas dataframe to CSV and writes it out to a local folder.\n",
    "        Parameters:\n",
    "            pd_df (pd.Dataframe): The pandas data frame of the data\n",
    "            path (string): the path of the local \"out\" folder\n",
    "        Returns:\n",
    "            Boolean: True is written, False if not written \"\"\" \n",
    "    status = True\n",
    "\n",
    "    # If the csv dir isn't there, make it\n",
    "    csv_dir = out_path\n",
    "    if not os.path.exists(csv_dir):\n",
    "        os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        df.to_csv(csv_dir, index=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "    return status                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def entry_point(data_url):\n",
    "#     urls_gen = find_csv_urls(data_url)\n",
    "#     gap_filler_dict = (get_mapping_dicts(gap_filler_yam_path) \n",
    "#     header_mapping_dict = (get_mapping_dicts(header_mapping_yam_path))\n",
    "\n",
    "#     for _url in urls_gen:\n",
    "#         data_name = extract_name(_url)\n",
    "#         df = csvs_to_pandas(_url)\n",
    "#         if not df:\n",
    "#             continue\n",
    "#         df = fill_gaps(df, gap_filler_dict)\n",
    "#         df = standardise_headers(df)\n",
    "#         write_csv(df, out_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     entry_point(data_url=remote_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_random_values(df, holes=20):\n",
    "    \"\"\"\n",
    "    Smashes holes in your dataframe to the approximate number that you\n",
    "    request (randint might choose the same cell twice)\n",
    "    \"\"\"\n",
    "    for i in range(holes):\n",
    "        row = random.randint(1, df.shape[0]-1)\n",
    "        col = random.randint(0, df.shape[1]-1)\n",
    "        df.iloc[row, col] = float('nan')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-42-23159a82f037>\u001b[0m(14)\u001b[0;36mproof_of_concept\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     13 \u001b[1;33m    \u001b[1;31m# Creating a sample df. This is in place of csvs_to_pandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 14 \u001b[1;33m    \u001b[0msamp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsvsample_to_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpct\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     15 \u001b[1;33m    \u001b[1;31m# Testing get_mapping_dicts func. Creating filler dictioary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "[Errno 13] Permission denied: 'D:\\\\repo\\\\sdg_csvw\\\\sdg-csv-data-filler\\\\out\\\\test_df.csv'\n"
     ]
    }
   ],
   "source": [
    "test_yam_path = os.path.join(os.getcwd(), \"substitutions\", \"samp_filler.yaml\")\n",
    "csv_path = os.path.join(os.getcwd(), \"data\", \"indicator_9-1-1.csv\")\n",
    "dict_of_nonstandard_standard_path = (os.path.join(os.getcwd(),\n",
    "                                                  \"substitutions\",\n",
    "                                                  \"test_dict_of_nonstandard_standard.yaml\"))\n",
    "\n",
    "def proof_of_concept(csv_path,\n",
    "                     ns=dict_of_nonstandard_standard_path, \n",
    "                     path=test_yam_path,\n",
    "                     out_path=out_path,\n",
    "                     file_name=\"test_df.csv\"):\n",
    "    import ipdb; ipdb.set_trace()\n",
    "    # Creating a sample df. This is in place of csvs_to_pandas\n",
    "    samp_df = csvsample_to_pandas(path_to_file=csv_path, pct=1)\n",
    "    # Testing get_mapping_dicts func. Creating filler dictioary\n",
    "    samp_filler_dict = get_mapping_dicts(path)\n",
    "    # Creating dictionary to map non-standard terms with standard ones. \n",
    "    nonstandard_standard = get_mapping_dicts(ns)\n",
    "    # Creating gaps in the data (this will not be used in production)\n",
    "    holey_df = delete_random_values(samp_df)\n",
    "    # Testing that the filler dict works\n",
    "    refilled_df = holey_df.fillna(value=samp_filler_dict)\n",
    "    # Testing that standardise dict dills gaps as expected\n",
    "    samp_df = standardise_cell_values(refilled_df, nonstandard_standard)\n",
    "    #Writing the df to csv locally. \n",
    "    was_written = write_csv(samp_df, os.path.join(out_path, file_name))\n",
    "    return samp_df, was_written\n",
    "\n",
    "poc_df, was_written = proof_of_concept(csv_path, path=test_yam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "was_written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
