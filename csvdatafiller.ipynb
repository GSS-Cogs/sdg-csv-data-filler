{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import pyyaml?\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\"\"\"\n",
    "sdg-csv-data-filler is the first module in a data pipeline to take\n",
    "data from the SDG data repo and make it exportable as CSVW.\n",
    "\"\"\"\n",
    "\n",
    "# setting paths to directories and files\n",
    "remote_data_url = \"https://github.com/ONSdigital/sdg-data/tree/develop/data\"\n",
    "cwd = os.getcwd()\n",
    "data_path = os.path.join(cwd, 'data')\n",
    "out_path = os.path.join(cwd, 'out')\n",
    "gap_filler_yam_path = (os.path.join\n",
    "                       (data_path,\n",
    "                        \"gap_filler.yaml\"))\n",
    "header_mapping_yam_path = (os.path.join\n",
    "                           (data_path,\n",
    "                            \"header_mapping.yaml\"))\n",
    "\n",
    "def get_mapping_dicts(gap_filler_yaml, header_mapping_yaml):\n",
    "    \"\"\"\n",
    "    Loads dictionaries for the gap filling and mapping of column names \n",
    "    from locally stored .yaml files\n",
    "    \n",
    "        Parameters:\n",
    "            gap_filler_yaml (string): Path to the yaml file storing the values\n",
    "                to fill gaps with in each column\n",
    "            header_mapping_yaml (string): Path to the yaml file storing the \n",
    "                names to change headers to for each column\n",
    "        Returns:\n",
    "            dict: gap filler dict\n",
    "            dict: header mapping dict\n",
    "            \n",
    "    \"\"\"\n",
    "    with open(gap_filler_yaml) as file:\n",
    "        gap_filler_dict = yaml.safe_load(file)\n",
    "    with open(header_mapping_yaml) as file:\n",
    "        header_mapping_dict = yaml.safe_load(file)\n",
    "    return gap_filler_dict, header_mapping_dict\n",
    "\n",
    "def find_csv_urls(url):\n",
    "    \"\"\"\n",
    "    Provided with a data folder URL, this function finds the URLS\n",
    "    of the CSV files within the folder. A generator is yielded with\n",
    "    the links of all files in the folder.\n",
    "        Parameters:\n",
    "            url (string): the URL of the repo/folder which contains\n",
    "                the CSV files to be captured\n",
    "        Yields:\n",
    "            string: generator, the next URL for the CSV file in the \n",
    "            remote data folder \n",
    "    \"\"\"\n",
    "    soup = bs(page)\n",
    "    csv_link_pattern = r\"\\/ONSdigital\\/sdg-data\\/blob\\/develop\\/data\\/indicator_\\d-\\d{1,2}-\\d{1,2}\\.csv\"\n",
    "    to_repl_pattern = r\"\\/sdg-data\\/blob\\/develop\"\n",
    "    replacement_pattern = \"/sdg-data/develop\"\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(pattern)}):\n",
    "        link = link.get('href')\n",
    "        link = re.sub(to_repl_pattern, replacement_pattern, link)\n",
    "        yield (\"https://raw.githubusercontent.com\"+link)\n",
    "\n",
    "def extract_name(url):\n",
    "    \"\"\"\n",
    "    Extracts the name of the dataset from the url, for use later when\n",
    "        saving the modified dataset locally.\n",
    "    \n",
    "        Parameters:\n",
    "            url (string): the URL of the remotely hosted csv file \n",
    "            being captured\n",
    "        Returns:\n",
    "            string: the name of the dataset\n",
    "    \"\"\"\n",
    "    capture_all = re.match(patt_catch, url)\n",
    "    data_name = (capture_all.group(1))\n",
    "    return data_name\n",
    "                       \n",
    "def csvs_to_pandas(url):\n",
    "    \"\"\"\n",
    "    Provided with a URL of a file, the fucntion will check if the CSV\n",
    "    is populated and if not empty return a Pandas dataframe of the CSV\n",
    "        Parameters:\n",
    "            url (string): the URL of a CSV file to be captured\n",
    "        Returns:\n",
    "            pd.DataFrame: a Pandas dataframe of the CSV\n",
    "    \"\"\"\n",
    "    if \"no data for this indicator yet\" in bs(requests.get(url)).text:\n",
    "        return None\n",
    "    else:\n",
    "        return pd.read_csv(csv_url)\n",
    "                       \n",
    "def fill_gaps(pd_df, gap_filler_dict):\n",
    "    \"\"\"\n",
    "    Given a Pandas dataframe and a dictionary containing the column names\n",
    "    the correct 'fillers' for each column, this function will fill\n",
    "    each column with the correct values when empty cells are found.\n",
    "        Parameters:\n",
    "            pd_df (pd.Dataframe): the variable to which the dataframe \n",
    "                containing the csv data is assigned\n",
    "            gap_filler_dict (dict): a dictionary with column name and value \n",
    "                to fill gaps as key value pairs, e.g.\n",
    "                {\"Age\":\"All\",\"Sex\":\"T\"}\n",
    "        Returns:\n",
    "            pd.Dataframe: A dataframe with all cells full\"\"\"\n",
    "    df = pd_df.fillna(gap_filler_dict, axis=1)\n",
    "    return df\n",
    "\n",
    "def standardise_cell_values(pd_df, dict_of_nonstandard_standard):\n",
    "    \"\"\"\n",
    "    Maps non-standard values e.g. \"Males\" to standard values like \"M\".\n",
    "    Mapping is carried out on a column-specific basis.\n",
    "    \"\"\"\n",
    "    df = (pd_df.replace\n",
    "          (to_replace=dict_of_nonstandard_standard,\n",
    "          value=None))\n",
    "    return df\n",
    "                       \n",
    "def standardise_headers(pd_df, dict_of_nonstandard_standard):\n",
    "    \"\"\"\n",
    "    Changes the non-standard CSV column headers to harmonised-data \n",
    "    column headers. \n",
    "    e.g. 'Age Group' to 'Age'\n",
    "    \"\"\"\n",
    "    # Do whitespaces need to be replaced with underscores?\n",
    "    col_names = list(pd_df.columns)\n",
    "    correction_dict = ({col_nm: col_nm.strip().title() \n",
    "                        for col_nm in col_names})\n",
    "    df = pd_df.rename(columns=correction_dict)\n",
    "    return df\n",
    "                       \n",
    "def write_csv(pd_df, out_path):\n",
    "    \"\"\"\n",
    "    Converts a Pandas dataframe to CSV and writes it out to a local folder.\n",
    "        Parameters:\n",
    "            pd_df (pd.Dataframe): The pandas data frame of the data\n",
    "            path (string): the path of the local \"out\" folder\n",
    "        Returns:\n",
    "            writes csv \"\"\" #I am unsure how to express this output\n",
    "    #code adapted from https://github.com/open-sdg/sdg-build/blob/797c7848d48de122d9eddfff6b2c8a9898f7e225/sdg/data.py#L35\n",
    "#     def write_csv(inid, df, ftype='data', site_dir=''):\n",
    "#     \"\"\"\n",
    "#     For a given ID and data set, write out as csv\n",
    "#     Args:\n",
    "#         inid: str. The indicator identifier\n",
    "#         df: DataFrame. \n",
    "#         ftype: Sets directory path\n",
    "#         site_dir: str. The site directory to build to.\n",
    "#     Returns:\n",
    "#         bool: Status\n",
    "#     \"\"\"\n",
    "    status = True\n",
    "\n",
    "    # If the csv dir isn't there, make it\n",
    "    csv_dir = out_path\n",
    "    if not os.path.exists(csv_dir):\n",
    "        os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        df.to_csv(csv_dir, index=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "    return status                   \n",
    "\n",
    "def entry_point(data_url):\n",
    "    urls_gen = find_csv_urls(data_url)\n",
    "    gap_filler_dict, header_mapping_dict = (get_mapping_dicts\n",
    "                                            (gap_filler_yam_path, \n",
    "                                             header_mapping_yam_path))\n",
    "    for _url in urls_gen:\n",
    "        data_name = extract_name(_url)\n",
    "        df = csvs_to_pandas(_url)\n",
    "        if not df:\n",
    "            continue\n",
    "        df = fill_gaps(df, gap_filler_dict)\n",
    "        df = standardise_headers(df)\n",
    "        write_csv(df, out_path)\n",
    "\n",
    "entry_point(data_url=remote_data_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
